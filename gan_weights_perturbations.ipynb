{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan-weights-perturbations.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLoZFl65nEoGR84N2Gw828"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikVBSG1aTeCa"
      },
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install pytorch-pretrained-biggan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGaTkFC1XhNp"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
        "                                       save_as_images, display_in_terminal)\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "#import logging\n",
        "## Seems to be needed, not sure why (one_hot_from_names?)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "model = BigGAN.from_pretrained('biggan-deep-256')\n",
        "\n",
        "# Prepare a input\n",
        "truncation = 0.4\n",
        "class_vector = one_hot_from_names(['soap bubble', 'coffee', 'cock', 'boathouse'], batch_size=4)\n",
        "noise_vector = truncated_noise_sample(truncation=truncation, batch_size=4, seed=11)\n",
        "\n",
        "# All in tensors\n",
        "noise_vector = torch.from_numpy(noise_vector)\n",
        "class_vector = torch.from_numpy(class_vector)\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "noise_vector = noise_vector.to('cuda')\n",
        "class_vector = class_vector.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Generate an image\n",
        "with torch.no_grad():\n",
        "    output0 = model(noise_vector, class_vector, truncation)\n",
        "\n",
        "# If you have a GPU put back on CPU\n",
        "output0 = output0.to('cpu')\n",
        "\n",
        "# If you have a sixtel compatible terminal you can display the images in the terminal\n",
        "# (see https://github.com/saitoha/libsixel for details)\n",
        "#display_in_terminal(output)\n",
        "\n",
        "# Save results as png images\n",
        "save_as_images(output0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN-99j3uXq7W"
      },
      "source": [
        "def rescale_parameters(model, weights_only = True, mult_noise = True, alpha = 0.3, add_noise = False, sigma = 0.0, noise_dist = 'normal', normalize = False):\n",
        "    c = 0\n",
        "\n",
        "    if noise_dist==\"laplace\":\n",
        "        m = torch.distributions.laplace.Laplace(0, 1)\n",
        "    # We can also set m to the normal distribution, avoiding many lines of code below.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for name, W in model.named_parameters():\n",
        "            if weights_only and not 'weight' in name:\n",
        "                continue\n",
        "            c = c + 1\n",
        "            if mult_noise:\n",
        "                if noise_dist == 'normal':\n",
        "                    W.copy_(W*(1 + alpha*torch.randn(W.shape, device='cuda')))\n",
        "                elif noise_dist == 'laplace':\n",
        "                    W.copy_(W*(1 + alpha*m.sample(W.shape).to('cuda') ))  \n",
        "                else:\n",
        "                    raise Exception(\"Noise distribution not implemented: \"+noise_dist)\n",
        "                if normalize:\n",
        "                    W.copy_(W/np.sqrt(1+alpha**2))\n",
        "            if add_noise:\n",
        "                if noise_dist == 'normal':\n",
        "                    W.copy_(W + sigma*torch.randn(W.shape, device='cuda') )\n",
        "                elif noise_dist == 'laplace':\n",
        "                    W.copy_(W + sigma*m.sample(W.shape).to('cuda'))\n",
        "                else:\n",
        "                    raise Exception(\"Noise distribution not implemented: \"+noise_dist)\n",
        "                if normalize:\n",
        "                    varW  = torch.var(W)\n",
        "                    coeff = torch.sqrt(varW/(varW + sigma**2))\n",
        "                    W.copy_(W*coeff)\n",
        "\n",
        "    print(f\"Number of layers affected: {c}.\")\n",
        "\n",
        "\n",
        "netGAN = BigGAN.from_pretrained('biggan-deep-256')\n",
        "netGAN.to('cuda')\n",
        "\n",
        "torch.manual_seed(8)\n",
        "\n",
        "rescale_parameters(netGAN, alpha = 0.25, normalize = False, add_noise = False, sigma=0.0)#, noise_dist='laplace')#, weights_only=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = netGAN(noise_vector, class_vector, truncation)\n",
        "\n",
        "output = output.to('cpu')\n",
        "save_as_images(output);\n",
        "\n",
        "pic_size = torch.numel(output)\n",
        "l2_dist_norm = torch.sqrt( torch.sum( (output - output0)**2)/pic_size )\n",
        "print(l2_dist_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFKX3x1ebUWa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}